{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUSGXnmpOQQ8VMnRmFaVdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangeethapk/AI-ML/blob/main/IntermediateAssesment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processing trining dataset"
      ],
      "metadata": {
        "id": "f4EC6L5o-xqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data\n",
        "import pandas as pd\n",
        "train_df=pd.read_csv('/content/train_LZdllcl.csv')\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "ijyWviVS-w_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop user id\n",
        "train_df.drop('employee_id',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "IG56_JKOVSku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGkGQZ6BxdFq"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.boxplot()"
      ],
      "metadata": {
        "id": "ssyRBKeSGjJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skewness_values = train_df.select_dtypes(include=['number']).skew()\n",
        "print(skewness_values)"
      ],
      "metadata": {
        "id": "AHd3uKNeOwlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Copy to avoid changing original\n",
        "df = train_df.copy()\n",
        "\n",
        "# List of skewed columns to cap (based on your skewness results)\n",
        "skewed_columns = [\n",
        "    'no_of_trainings',\n",
        "    'length_of_service',\n",
        "    'awards_won?',\n",
        "    'age'\n",
        "]\n",
        "\n",
        "for col in skewed_columns:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_cap = Q1 - 1.5 * IQR\n",
        "    upper_cap = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Apply capping\n",
        "    df[col] = df[col].clip(lower=lower_cap, upper=upper_cap)\n",
        "\n",
        "# Re-check skewness after capping\n",
        "new_skewness = df[skewed_columns].skew()\n",
        "print(\"Skewness after IQR capping:\")\n",
        "print(new_skewness)\n"
      ],
      "metadata": {
        "id": "jBYQ63tYKo0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df['length_of_service'] = np.log1p(df['length_of_service'])\n",
        "df['age'] = np.log1p(df['age'])\n",
        "print(df[['length_of_service', 'age']].skew())\n",
        "\n"
      ],
      "metadata": {
        "id": "G3jPKlcaLH_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split features and target"
      ],
      "metadata": {
        "id": "ew11EwNSL_eE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get X and y value\n",
        "X=df.drop('is_promoted',axis=1)\n",
        "y=df['is_promoted']\n",
        "#"
      ],
      "metadata": {
        "id": "enJ8WuFTfMzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find imbalance in y\n",
        "y.value_counts()"
      ],
      "metadata": {
        "id": "8aGx1ehMfg12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "lN9NLLNjf43s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove NUll value from X_train"
      ],
      "metadata": {
        "id": "Tc2-Jb2HlFA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Handle missing values in X_train\n",
        "X_train.isnull().sum()"
      ],
      "metadata": {
        "id": "Xqu-OSZLPNIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#impute missing values' most frequent' value for education\n",
        "mode_education=X_train['education'].mode()[0]\n",
        "X_train['education']=X_train['education'].fillna(mode_education)\n",
        "#impute previous_year_rating\n",
        "X_train['previous_year_rating']=X_train['previous_year_rating'].fillna(X_train['previous_year_rating'].median())\n"
      ],
      "metadata": {
        "id": "wLdKUXb_PYXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.isnull().sum()"
      ],
      "metadata": {
        "id": "eHOmsOasP5GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove null value from X_test using smae method"
      ],
      "metadata": {
        "id": "iPrWVZwVlN14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.isnull().sum()"
      ],
      "metadata": {
        "id": "evoZi7omP9Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#impute missing values' most frequent' value for education\n",
        "mode_education=X_test['education'].mode()[0]\n",
        "X_test['education']=X_test['education'].fillna(mode_education)\n",
        "#impute previous_year_rating\n",
        "X_test['previous_year_rating']=X_test['previous_year_rating'].fillna(X_test['previous_year_rating'].median())"
      ],
      "metadata": {
        "id": "7XXa7Z4ERvEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.isnull().sum()"
      ],
      "metadata": {
        "id": "VvVsxXyDR7sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode"
      ],
      "metadata": {
        "id": "nIejuVFplZI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#encode X_train and X_test data using for loop for all cat values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "for col in X_train.select_dtypes(include=['object']):\n",
        "  X_train[col]=le.fit_transform(X_train[col])\n",
        "  X_test[col]=le.transform(X_test[col])\n",
        "\n"
      ],
      "metadata": {
        "id": "I_O0MsP1R8j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "Di-OZDnqTkNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale"
      ],
      "metadata": {
        "id": "2vGTL-CSldd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scale X_train and X_test numeric values\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler=StandardScaler()\n",
        "for col in X_train.select_dtypes(include=['number']):\n",
        "  X_train[col]=scaler.fit_transform(X_train[[col]])\n",
        "  X_test[col]=scaler.transform(X_test[[col]])\n"
      ],
      "metadata": {
        "id": "53O07mEQUb-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE"
      ],
      "metadata": {
        "id": "V-tcBam9lgMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Smote\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote=SMOTE()\n",
        "X_train,y_train=smote.fit_resample(X_train,y_train)"
      ],
      "metadata": {
        "id": "EA6AUSb9V9Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the models"
      ],
      "metadata": {
        "id": "Q8oZU_OkjD8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "\n",
        "\n",
        "# List of models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    'LightGBM': LGBMClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Use scaled data for models that are sensitive to feature scale\n",
        "    if name in ['SVM', 'Logistic Regression', 'KNN', 'Naive Bayes']:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "        'F1 Score': f1_score(y_test, y_pred, zero_division=0)\n",
        "    })\n",
        "\n",
        "#. Show results as table\n",
        "results_df = pd.DataFrame(results).sort_values(by='F1 Score', ascending=False)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKGvSzyPjGNY",
        "outputId": "821322bc-0cbe-40da-c106-e27e178321de"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:42:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 40086, number of negative: 40086\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2062\n",
            "[LightGBM] [Info] Number of data points in the train set: 80172, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "                 Model  Accuracy  Precision    Recall  F1 Score\n",
            "8             LightGBM  0.942073   0.884507  0.345815  0.497229\n",
            "7              XGBoost  0.938880   0.790244  0.356828  0.491654\n",
            "1        Decision Tree  0.899197   0.406812  0.473568  0.437659\n",
            "2        Random Forest  0.920909   0.534338  0.351322  0.423920\n",
            "3    Gradient Boosting  0.829502   0.265037  0.596916  0.367084\n",
            "4                  SVM  0.724138   0.208057  0.830396  0.332745\n",
            "6                  KNN  0.809433   0.228006  0.545154  0.321533\n",
            "0  Logistic Regression  0.707809   0.181161  0.718062  0.289328\n",
            "5          Naive Bayes  0.697044   0.179378  0.743392  0.289017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tunning"
      ],
      "metadata": {
        "id": "tMpPI_GZoKBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "# Models & Hyperparameter Grids\n",
        "model_params = {\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(max_iter=1000),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'solver': ['liblinear', 'lbfgs']\n",
        "        }\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'model': DecisionTreeClassifier(),\n",
        "        'params': {\n",
        "            'max_depth': [5, 10, 15],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100],\n",
        "            'max_depth': [10, 20, None]\n",
        "        }\n",
        "    },\n",
        "    'SVM': {\n",
        "        'model': SVC(),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear', 'rbf']\n",
        "        }\n",
        "    },\n",
        "    'KNN': {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'n_neighbors': [3, 5, 7]\n",
        "        }\n",
        "    },\n",
        "    'Naive Bayes': {\n",
        "        'model': GaussianNB(),\n",
        "        'params': {}  # No hyperparams to tune for basic NB\n",
        "    }\n",
        "}\n",
        "\n",
        "#  Train & Evaluate\n",
        "results = []\n",
        "\n",
        "for name, mp in model_params.items():\n",
        "    grid = GridSearchCV(mp['model'], mp['params'], cv=5, scoring='f1', n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Best Params': grid.best_params_,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "        'F1 Score': f1_score(y_test, y_pred, zero_division=0)\n",
        "    })\n",
        "\n",
        "# 4. Display Results\n",
        "results_df = pd.DataFrame(results).sort_values(by='F1 Score', ascending=False)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "R7vQXSGTi8bj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}